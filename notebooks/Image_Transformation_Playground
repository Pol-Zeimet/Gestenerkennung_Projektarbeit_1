{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image_Transformation_Playground","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1D7Ft0YpG-EMkNzQMkGbfxyLpYuPDgshy","authorship_tag":"ABX9TyMIobR+FfB09yS238oZD9ku"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gg_JkORM4tha","colab_type":"text"},"source":["Notable mention for inspiring me on how to use tf-pose in colab\n","https://colab.research.google.com/drive/1kUVQSmWSJ3aBpbh83NNbUHbEA0IQqufy#scrollTo=YksVb2TvuzR7&forceEdit=true&sandboxMode=true\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Gu5I96F3Mw9-","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HDLDIlY8lqR1","colab_type":"text"},"source":["Cleaning up unwanted Data\n"]},{"cell_type":"code","metadata":{"id":"C1yGCnreR3cG","colab_type":"code","colab":{}},"source":["!rm -r sample_data/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9yv9e973AuP","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","!bash \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/initialize.sh\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-60n35nVP--y","colab_type":"code","colab":{}},"source":["!pip uninstall -y keras\n","!pip uninstall -y scipy\n","!pip install scipy==1.2.2\n","!pip install keras==2.0.8\n","!pip install Cython"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDr5xi4g38hS","colab_type":"text"},"source":["Installing TensorRT in Colab environment\n"]},{"cell_type":"code","metadata":{"id":"tBaGa1jYq-R-","colab_type":"code","colab":{}},"source":["!bash \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/install_tensorRT.sh\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mMuPOdpbQORe","colab_type":"text"},"source":["Installing Coco\n"]},{"cell_type":"code","metadata":{"id":"kdG3G9PfQJ-V","colab_type":"code","colab":{}},"source":["!git clone https://github.com/waleedka/coco\n","!pip install -U setuptools\n","!pip install -U wheel\n","!make install -C coco/PythonAPI"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVMMnbZlFEg_","colab_type":"text"},"source":["Restart The Notebook now."]},{"cell_type":"code","metadata":{"id":"AcKzRBYLFEOm","colab_type":"code","colab":{}},"source":["import os\n","\n","os.kill(os.getpid(), 9)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pr2xkNu-K0nC","colab_type":"code","colab":{}},"source":["cd /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXNjq8kUCqvT","colab_type":"code","colab":{}},"source":["!unzip /content/data.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7qLqdLIyNT8","colab_type":"code","colab":{}},"source":["cd \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/src/movement_classification\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMjYNVlGt1Zc","colab_type":"code","colab":{}},"source":["import os\n","import math \n","import sys\n","import glob\n","\n","\n","sys.path.append(\"../../thirdparty/tf-pose/tf-pose-estimation\")\n","\n","\n","import random\n","import math\n","import numpy as np\n","import skimage.io\n","import matplotlib\n","import matplotlib.pyplot as plt\n"," \n","import logging\n","import traceback\n","\n","import time\n","import cv2\n","\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","%matplotlib inline \n","\n","print(\"Environment Ready\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9tauuPeAHs1","colab_type":"code","colab":{}},"source":["#https://github.com/shahroudy/NTURGB-D/issues/39\n","# convert from rgb to depth frame\n","rgb_to_depth_affine_transforms = dict(\n","   C001=np.array([[3.45638311e-01,  2.79844266e-03, -8.22281898e+01],\n","                  [-1.37185375e-03, 3.46949734e-01,  1.30882644e+01],\n","                  [0.00000000e+00, 0.00000000e+00,  1.00000000e+00]]),\n"," \n","   C002=np.array([[3.42938209e-01,  8.72629655e-04, -7.28786114e+01],\n","                  [3.43287830e-04,  3.43578203e-01,  1.75767495e+01],\n","                  [0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]),\n","\n","   C003=np.array([[3.45121348e-01,  8.53232038e-04, -7.33328852e+01],\n","                  [1.51167845e-03,  3.45115132e-01,  2.22178592e+01],\n","                  [0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]))\n","\n","\n","#-------------------------------------------------------------------------\n","# convert from depth to rgb frame\n","depth_to_rgb_affine_transforms = dict(\n","    C001=np.array([[2.89310518e+00, -2.33353370e-02,  2.38200221e+02],\n","                   [1.14394588e-02,  2.88216964e+00, -3.67819523e+01],\n","                   [0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]),\n","\n","    C002=np.array([[2.90778446e+00, -1.04633946e-02,  2.15505801e+02],\n","                   [-3.43830682e-03,  2.91094100e+00, -5.13416831e+01],\n","                   [0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]),\n","\n","    C003=np.array([[2.89756295e+00, -7.16367761e-03,  2.12645813e+02],\n","                   [-1.26919485e-02,  2.89761514e+00, -6.53095423e+01],\n","                   [0.00000000e+00,  0.00000000e+00,  1.00000000e+00]]))\n","\n","#usage:\n","#depth_to_rgb_affine_transform = depth_to_rgb_affine_transforms[\"C001\"]\n","#frame_depth = cv2.warpAffine(frame_depth, depth_to_rgb_affine_transform[:2, :], (1920, 1080))\n","\n","#rgb_to_depth_affine_transform = rgb_to_depth_affine_transforms[\"C001\"]\n","#frame_rgb = cv2.warpAffine(frame_rgb, rgb_to_depth_affine_transform[:2, :], (512, 424))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tn9O9P1t6BQM","colab_type":"code","colab":{}},"source":["rgb_distort = np.asarray([ 2.0429028189430772e-02, -7.8399448855923665e-03,\n","      -3.0949667668667908e-03, 2.1848647826377197e-03,\n","      -5.2344198896187882e-02 ])\n","rgb_cam_mat = np.asarray([[ 1.0327407219495085e+03, 0., 9.5685930301206076e+02],\n","                [0.,1.0323427647512485e+03, 5.3914133979587950e+02],\n","                [0., 0., 1. ]])\n","fundamental_mat = np.asarray([[-2.6636940145645524e-09, -1.4958501248061541e-06, 9.0258021041470491e-04],\n","                          [1.8777442900559247e-06,-3.2383398793910185e-08, -2.4312498831141331e-02],\n","                          [-8.0783455537693954e-04, 1.6314744604634310e-02, 1. ]])\n","essential_mat = np.asarray([[6.3896685404108684e-06, 3.5868622103996167e-03, 2.1730914533345201e-04],\n","                          [-4.4999913034017682e-03, 7.7576515565382328e-05, 5.2288622153514326e-02],\n","                            [1.7299684019311893e-04, -5.2359308528883181e-02,  7.8008494633390984e-05]])\n","rotation_mat =np.asarray([[ 9.9984755688188409e-01, -7.2421827552744493e-04,1.7445300376233552e-02],\n","                          [7.4908907566518894e-04,9.9999871243524374e-01, -1.4191523566401902e-03],\n","                          [-1.7444250138207187e-02, 1.4320041005633978e-03,9.9984681201740677e-01 ]])\n","translation_vector = np.asarray( [ -5.2359222228778743e-02, -2.1225091776675036e-04, -3.5871707732988696e-03 ])\n","depth_distort = np.asarray([8.8866838848423224e-02, -2.4742250848813654e-01, -2.8648128998355381e-03, -6.6909365375191258e-04, 5.7299947210392929e-02 ])\n","depth_cam_mat = np.asarray([[ 7.1162970910343665e+02, 0., 5.0990097114150876e+02], \n","                   [0.,7.1094384711878797e+02, 4.0360735428585957e+02], \n","                   [0.,0., 1. ]])\n","  \n","def draw_human_v2(npimg, upscaled_depth_image, depth_image, human, imgcopy=False):\n","        if imgcopy:\n","            npimg = np.copy(npimg)\n","        image_h, image_w = npimg.shape[:2]\n","        centers = {}\n","        \n","        for i in range(common.CocoPart.Background.value):\n","            if i not in human.body_parts.keys():\n","                continue\n","            body_part = human.body_parts[i]\n","            center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n","           \n","            #transformed_center = [body_part.x* image_w ,body_part.y* image_h, 0.] @ essential_mat\n","            transformed_center, jac = cv2.projectPoints(np.asarray([body_part.x* image_w, body_part.y* image_h, 0.]), rotation_mat, translation_vector, rgb_cam_mat, rgb_distort)\n","            cv2.circle(npimg, center, 2, common.CocoColors[i], thickness=1, lineType=4, shift=0)\n","            cv2.circle(upscaled_depth_image, center, 2, common.CocoColors[i], thickness=1, lineType=4, shift=0)\n","            print('center: ' + str(center))\n","            cv2.circle(depth_image, (int(transformed_center[0][0][0]),int(transformed_center[0][0][1])), 2, common.CocoColors[i], thickness=1, lineType=4, shift=0)\n","\n","        return npimg, upscaled_depth_image, depth_image\n","\n","\n","def load_depth_images(path, camera):\n","  depth_to_rgb_affine_transform  = depth_to_rgb_affine_transforms[camera]\n","  depth_frames= []\n","  raw_depth_frames=[]\n","  files = glob.glob(path +'*.png')\n","  files.sort()\n","  for file in files:\n","    raw_depth = cv2.imread(file)\n","    h,  w = raw_depth.shape[:2]\n","    newcameramtx, roi=cv2.getOptimalNewCameraMatrix(depth_cam_mat,depth_distort,(w,h),1,(w,h))\n","    cv2.undistort(raw_depth, depth_cam_mat, depth_distort, None, newcameramtx)\n","    depth_frames.append(cv2.warpAffine(raw_depth, depth_to_rgb_affine_transform[:2, :], (1920, 1080)))\n","    raw_depth_frames.append(raw_depth)\n","\n","  print('read ' + str(len(depth_frames)) + ' images' )\n","  return depth_frames, raw_depth_frames\n","\n","def str2bool(v):\n","    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n","\n","\n","def run_inference(filename, depth_frames,raw_depth_frames, rgb_out, depth_out, raw_depth_out):\n","  \n","  frames = []\n","  capture = cv2.VideoCapture(filename)\n","  frame_array = None\n","  try:\n","    i = 0\n","    while i < len(depth_frames):\n","        frame = []\n","        ret, color_image = capture.read()\n","        h,  w = color_image.shape[:2]\n","        newcameramtx, roi=cv2.getOptimalNewCameraMatrix(rgb_cam_mat,rgb_distort,(w,h),1,(w,h))\n","        color_image = cv2.undistort(color_image, rgb_cam_mat, rgb_distort, None, newcameramtx)\n","        depth_frame = depth_frames[i]\n","        raw_depth_frame = raw_depth_frames[i]\n","        if depth_frame is None:\n","          break\n","        \n","        logger.info('reading frame: ' +  str(i))\n","        \n","        humans = estimator_obj.inference(color_image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n","\n","        logger.info('postprocess+')\n","        if len(humans) > 0:\n","            human = humans[0]\n","            color_image, depth_frame,raw_depth_frame = draw_human_v2(color_image, depth_frame,raw_depth_frame, human)\n","            depth_out.write(depth_frame)\n","            raw_depth_out.write(raw_depth_frame)\n","            rgb_out.write(color_image)\n","            raw_depth_out\n","        fps_time = time.time()\n","        logger.info('finished+')\n","        i+=1\n","  except Exception as err:\n","      logger.critical(err)\n","      traceback.print_exc() \n","      pass \n","  finally:\n","    rgb_out.release()\n","    depth_out.release()\n","    raw_depth_out.release()\n","    capture.release()\n","    capture = None\n","    rgb_out = None\n","    depth_out = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nmYd_q9OsPM","colab_type":"code","colab":{}},"source":["logger = logging.getLogger('TfPoseEstimator-WebCam')\n","logger.setLevel(logging.INFO)\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.DEBUG)\n","formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n","ch.setFormatter(formatter)\n","logger.addHandler(ch)\n","\n","fps_time = 0\n","\n","#Params for tf-pose\n","resize = '432x368' #if provided, resize images before they are processed. default is 0x0, Recommends : 432x368 or 656x368 or 1312x736\n","model = 'mobilenet_v2_large' # cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small'\n","tensorrt = 'True' #'for tensorrt process.'\n","show_process = 'False' #for debug purpose, if enabled, speed for inference is dropped.'\n","resize_out_ratio = 1.5 #if provided, resize heatmaps before they are post-processed. default is 1.0\n","\n","logger.debug('initialization %s : %s' % (model, get_graph_path(model)))\n","w, h = model_wh(resize)\n","if w > 0 and h > 0:\n","    estimator_obj = TfPoseEstimator(get_graph_path(model), target_size=(w, h), trt_bool=str2bool(tensorrt))\n","else:\n","    estimator_obj = TfPoseEstimator(get_graph_path(model), target_size=(368, 368), trt_bool=str2bool(tensorrt))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"crLNWuWpRmoh","colab_type":"text"},"source":["Preparing to read video files"]},{"cell_type":"code","metadata":{"id":"ZLirVQP1jVOh","colab_type":"code","colab":{}},"source":["\n","  fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n","  res=(1920, 1080)\n","  res_depth=(512, 424)\n","  path = '/content/data/'\n","  rgb_video_files = glob.glob(path +'*.avi')\n","  results = []\n","  \n","  for video_file in rgb_video_files:\n","    \n","    depth_folder_name = video_file[0:-8]+'/'\n","    print(depth_folder_name)\n","    out_rgb = cv2.VideoWriter( '/content/test_rgb.mp4', fourcc, 30.0, res)\n","    out_depth = cv2.VideoWriter( '/content/test_depth.mp4', fourcc, 30.0, res)\n","    raw_out_depth = cv2.VideoWriter( '/content/test_rawdepth.mp4', fourcc, 30.0, res_depth)\n","  \n","    camera = video_file[-24:-20]\n","    print(camera)\n","    depth_frames, raw_depth_frames = load_depth_images(depth_folder_name, camera)\n","    if len(depth_frames) > 0:\n","      run_inference(video_file, depth_frames,raw_depth_frames, out_rgb, out_depth, raw_out_depth)\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mXaUaeAgjXLz","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}