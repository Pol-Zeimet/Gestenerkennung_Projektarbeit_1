{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"movement_classifier_colab_with_tensorRT","provenance":[],"private_outputs":true,"collapsed_sections":[],"mount_file_id":"1BSO6EHyvmu8-fdXysH1dyxHwc0xmpswq","authorship_tag":"ABX9TyN6PgSumaWXEz6e48ivv7a/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gg_JkORM4tha","colab_type":"text"},"source":["Notable mention for inspiring me on how to use tf-pose in colab\n","https://colab.research.google.com/drive/1kUVQSmWSJ3aBpbh83NNbUHbEA0IQqufy#scrollTo=YksVb2TvuzR7&forceEdit=true&sandboxMode=true\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Gu5I96F3Mw9-","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9yv9e973AuP","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x\n","!bash \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/initialize.sh\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cDr5xi4g38hS","colab_type":"text"},"source":["Installing TensorRT in Colab environment\n"]},{"cell_type":"code","metadata":{"id":"tBaGa1jYq-R-","colab_type":"code","colab":{}},"source":["!bash \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/install_tensorRT.sh\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K7qLqdLIyNT8","colab_type":"code","colab":{}},"source":["cd \"/content/drive/My Drive/Colab Notebooks/ss19_pa_gesturerecognition_timeconv/src/movement_classification\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMjYNVlGt1Zc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLLwXE3O59CS","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append(\"../thirdparty/tf-pose/tf-pose-estimation\")\n","\n","import logging\n","import traceback\n","\n","import time\n","\n","import numpy as np\n","\n","import pyrealsense2 as rs\n","import cv2\n","\n","from tf_pose import common\n","from tf_pose.estimator import TfPoseEstimator\n","from tf_pose.networks import get_graph_path, model_wh\n","\n","import tensorflow as tf\n","print(tf.__version__)\n","print(\"Environment Ready\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tn9O9P1t6BQM","colab_type":"code","colab":{}},"source":["def getKeyPointCoords(keypoints_in, depth_frame, depth_colormap):\n","    keypoints_cam = []\n","    keypoints_out = []\n","    image_h, image_w = depth_colormap.shape[:2]\n","    base_keypoint = None\n","    if 8 in keypoints_in.keys() and 11 in keypoints_in.keys():\n","        base_x = int((keypoints_in[8][0] + keypoints_in[11][0])/2)\n","        base_y = int((keypoints_in[8][1] + keypoints_in[11][1])/2)\n","        base_z = (depth_frame.get_distance(keypoints_in[8][0], keypoints_in[8][1]) + depth_frame.get_distance(keypoints_in[11][0], keypoints_in[11][1]))/2.\n","        cv2.circle(depth_colormap, (base_x, base_y), 2, common.CocoColors[10], thickness=2, lineType=8)\n","        coords = transformPixelToCameraCoords([base_x, base_y, base_z])\n","        base_keypoint = [coords[0], coords[1], coords[2]]\n","        keypoints_cam.append(base_keypoint)\n","    \n","    if base_keypoint:\n","        for i in [0,1,2,3,4,5,6,7,8,11]:\n","            if i in keypoints_in.keys():\n","                z_pos = depth_frame.get_distance(keypoints_in[i][0], keypoints_in[i][1])\n","                coords = transformPixelToCameraCoords([keypoints_in[i][0], keypoints_in[i][1], z_pos]) \n","                keypoints_cam.append(coords)\n","            else:\n","                return None, depth_colormap\n","        rotation_matrix = np.column_stack(getBodyBaseVectors(keypoints_cam))\n","        for keypoint in keypoints_cam:\n","            keypoints_out.append(transformCameraToBodyCoords(rotation_matrix, base_keypoint, keypoint))\n","    else:\n","        return None, depth_colormap\n","    \n","    return keypoints_out, depth_colormap\n","\n","\n","\n","def transformPixelToCameraCoords(coords): # Pixel to Camera Coordinates\n","    return rs.rs2_deproject_pixel_to_point(depth_intrins, [coords[0], coords[1]], coords[2])\n","\n","\n","def transformCameraToBodyCoords(rotation_matrix, base_keypoint, coord):\n","    rc = np.array(coord)\n","    hc = np.array(base_keypoint)\n","    vector = rotation_matrix.dot(rc - hc)\n","    return vector\n","\n","\n","def getVector(coord_1, coord_2):\n","    a = np.array(coord_1)\n","    b = np.array(coord_2)\n","    return b - a\n","\n","def getBodyBaseVectors(keypoints):\n","    base_vector_x_norm = [0, 0, 0]\n","    base_vector_y_norm = [0, 0, 0]\n","    base_vector_x = getVector([keypoints[0][0],keypoints[0][1], keypoints[0][2]], [keypoints[10][0],keypoints[10][1], keypoints[10][2]])\n","    base_vector_y = getVector([keypoints[0][0],keypoints[0][1], keypoints[0][2]], [keypoints[2][0],keypoints[2][1], keypoints[2][2]])\n","   \n","    if np.linalg.norm(base_vector_x) != 0:\n","        base_vector_x_norm = base_vector_x / np.linalg.norm(base_vector_x)\n","            \n","    if np.linalg.norm(base_vector_y) != 0:\n","        base_vector_y_norm = base_vector_y / np.linalg.norm(base_vector_y)\n","\n","    base_vector_z_norm = np.cross(base_vector_x_norm, base_vector_y_norm)\n","    \n","    return base_vector_x_norm, base_vector_y_norm, base_vector_z_norm\n","\n","def str2bool(v):\n","    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n","\n","   \n","def draw_human_v2(npimg, human, imgcopy=False):\n","        if imgcopy:\n","            npimg = np.copy(npimg)\n","        image_h, image_w = npimg.shape[:2]\n","        centers = {}\n","        \n","        # draw point\n","        for i in range(common.CocoPart.Background.value):\n","            if i not in human.body_parts.keys():\n","                continue\n","            body_part = human.body_parts[i]\n","            center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n","            centers[i] = center\n","            cv2.circle(npimg, center, 2, common.CocoColors[i], thickness=1, lineType=4, shift=0)\n","        return npimg, centers\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3nmYd_q9OsPM","colab_type":"code","colab":{}},"source":["logger = logging.getLogger('TfPoseEstimator-WebCam')\n","logger.setLevel(logging.INFO)\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.DEBUG)\n","formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n","ch.setFormatter(formatter)\n","logger.addHandler(ch)\n","\n","fps_time = 0\n","depth_intrins = None\n","\n","#Params for tf-pose\n","resize = '432x368' #if provided, resize images before they are processed. default is 0x0, Recommends : 432x368 or 656x368 or 1312x736\n","model = 'mobilenet_v2_large' # cmu / mobilenet_thin / mobilenet_v2_large / mobilenet_v2_small'\n","tensorrt = 'True' #'for tensorrt process.'\n","movementDetector = \"../../models/movement_classifier_20190927-180110.h5\" # path to movement-detector model.\n","show_process = 'False' #for debug purpose, if enabled, speed for inference is dropped.'\n","resize_out_ratio = 1.5 #if provided, resize heatmaps before they are post-processed. default is 1.0\n","\n","logger.debug('initialization %s : %s' % (model, get_graph_path(model)))\n","w, h = model_wh(resize)\n","if w > 0 and h > 0:\n","    estimator_obj = TfPoseEstimator(get_graph_path(model), target_size=(w, h), trt_bool=str2bool(tensorrt))\n","else:\n","    estimator_obj = TfPoseEstimator(get_graph_path(model), target_size=(368, 368), trt_bool=str2bool(tensorrt))\n","\n","frame_buffer = [[],[],[],[],[],[],[],[],[],[],[]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e_Opr2HJOWhI","colab_type":"code","colab":{}},"source":["\n","\n","logger.debug('initialization %s : %s' % ('Movement Detector', movementDetector))\n","movement_detector = tf.keras.models.load_model(movementDetector)\n","logger.debug(movement_detector.summary())\n","\n","print(\"creating rs stuff\")\n","config = rs.config()\n","location = '../../data/recording_samples/videos/'\n","filename = \"recording20200420-104706\"\n","rs.config.enable_device_from_file(config, location + filename + \".bag\", repeat_playback=False)\n","pipeline = rs.pipeline()\n","align_to = rs.stream.color\n","align = rs.align(align_to)\n","\n","\n","if tensorrt == 'True':\n","    modelsuffix = \"_\" + model + '_tensorRT'\n","else:\n","   modelsuffix = \"_\" + model\n","\n","res=(640,480)\n","fourcc = cv2.VideoWriter_fourcc(*'MP4V') #codec\n","out = cv2.VideoWriter(location + filename + modelsuffix + '.mp4', fourcc, 15.0, res)\n","\n","print(\"setup complete.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9tauuPeAHs1","colab_type":"code","colab":{}},"source":["profile = pipeline.start(config)\n","playback = profile.get_device().as_playback()\n","playback.set_real_time(False)\n","\n","try:\n","  i = 0\n","  while True:\n","      frame = pipeline.wait_for_frames()\n","      playback.pause()\n","      logger.info('reading frame: ' +  str(i))\n","      aligned_frames = align.process(frame)\n","      \n","      depth_frame = aligned_frames.get_depth_frame()\n","      \n","      # calibriated internal parameters from RealSense\n","      depth_intrins = depth_frame.profile.as_video_stream_profile().intrinsics\n","      color_frame = aligned_frames.get_color_frame()\n","      \n","      if not depth_frame or not color_frame:\n","        continue\n","\n","      depth_image = np.asanyarray(depth_frame.get_data())\n","      color_image = np.asanyarray(color_frame.get_data())\n","\n","      # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n","      depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.2), cv2.COLORMAP_JET)\n","      \n","      \n","      logger.info('image process+')\n","      humans = estimator_obj.inference(color_image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n","      \n","      logger.info('postprocess+')\n","      if len(humans) > 0:\n","          human = humans[0]\n","          color_image, keypoints = draw_human_v2(color_image, human, imgcopy=False)\n","          keypoint_coords, depth_colormap = getKeyPointCoords(keypoints, depth_frame, depth_colormap)\n","          \n","          if keypoint_coords != None:\n","              if len(frame_buffer[10]) == 45:\n","                  for keypoint in range(11):\n","                      frame_buffer[keypoint] = np.delete(frame_buffer[keypoint], [0,1,2])\n","              for keypoint in range(len(keypoint_coords)):\n","                  frame_buffer[keypoint] = np.append(frame_buffer[keypoint], keypoint_coords[keypoint])   \n","\n","      prediction = [0]\n","      if len(frame_buffer[10]) == 45:\n","          data_in = np.asarray([np.expand_dims(frame_buffer, axis = 3)])\n","          prediction = movement_detector.predict(data_in)\n","      \n","      if prediction[0] == 1:\n","          logger.info('Boxing') \n","          cv2.putText(color_image,'Boxing',(10,400), cv2.FONT_HERSHEY_SIMPLEX, 4,(0,255,0),2,cv2.LINE_AA)            \n","      \n","      cv2.putText(color_image,\n","                  \"%f\" % (1.0 / (time.time() - fps_time)),\n","                  (10, 20),  cv2.FONT_HERSHEY_SIMPLEX, 1,\n","                  (0, 255, 0), 2)\n","      cv2.putText(color_image,\n","                  modelsuffix,\n","                  (10, 460),  cv2.FONT_HERSHEY_SIMPLEX, 1,\n","                  (0, 255, 0), 2)\n","      #images = np.hstack((color_image,  depth_colormap))\n","\n","      fps_time = time.time()\n","      if cv2.waitKey(1) == 27:\n","          break\n","      logger.info('finished+')\n","      #out.write(images)\n","      out.write(color_image)\n","\n","      playback.resume()   \n","      i+=1\n","except Exception as err:\n","    logger.critical(err)\n","    pass \n","finally:\n","  pipeline.stop()\n","  out.release()\n","  out = None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Qt37ILVO26j","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}